{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "import gensim\n",
    "import codecs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "# path = get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "path = '/home/junsoo/PycharmProjects/word2vec_sample/text8'\n",
    "corpus = codecs.open(path, \"r\", encoding='utf-8', errors='ignore').read()\n",
    "\n",
    "words = corpus.split()\n",
    "corpus = []\n",
    "sentence = ''\n",
    "for idx,word in enumerate(words):\n",
    "    idx += 1\n",
    "    sentence += word + ' '\n",
    "    if idx % 30 == 0:\n",
    "        corpus.append(sentence.strip())\n",
    "        #print(sentence)\n",
    "        sentence = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253855"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(corpus[0:10])\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\r\\t\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "V = len(tokenizer.word_index) + 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_embedddings = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "word_inputs = Input(shape=(1,), dtype='int32')\n",
    "w = Embedding(V, dim_embedddings)(word_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "(?, 1, 128)\n",
      "(?, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# context\n",
    "context_inputs = Input(shape=(1,), dtype='int32')\n",
    "context  = Embedding(V, dim_embedddings)(context_inputs)\n",
    "print(context_inputs.shape)\n",
    "print(context.shape)\n",
    "output_layer = Dot(axes=2)([w, context])\n",
    "\n",
    "print(output_layer.shape)\n",
    "output_layer = Reshape((1,), input_shape=(1, 1))(output_layer)\n",
    "output_layer = Activation('sigmoid')(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 1, 128)        1560960     input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 1, 128)        1560960     input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dot_9 (Dot)                      (None, 1, 1)          0           embedding_13[0][0]               \n",
      "                                                                   embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 1)             0           dot_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 1)             0           reshape_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3,121,920\n",
      "Trainable params: 3,121,920\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SkipGram = Model(inputs=[word_inputs, context_inputs], outputs=output_layer)\n",
    "SkipGram.summary()\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ doc -------\n",
      "[7018, 5098, 964, 1296, 6, 1863, 28, 1115, 3953]\n",
      "------- data -----\n",
      "[[5098, 7923], [6, 11838], [6, 4223], [5098, 1296], [1296, 5098], [1863, 12109], [1863, 6308], [3953, 5242], [6, 1115], [6, 6201], [1863, 8423], [964, 2063], [3953, 1863], [1863, 1296], [1296, 742], [5098, 10167], [7018, 6], [1115, 8097], [6, 3953], [3953, 1260], [6, 6202], [6, 1863], [7018, 208], [6, 4106], [5098, 1863], [1296, 1863], [6, 3968], [5098, 9750], [6, 1296], [1296, 1115], [1296, 3001], [6, 964], [5098, 11569], [28, 4770], [7018, 1296], [5098, 6606], [1115, 9708], [1863, 8220], [28, 372], [6, 8437], [964, 12037], [1296, 3121], [1296, 1330], [3953, 4356], [28, 3614], [5098, 10392], [7018, 5668], [1863, 851], [3953, 10478], [964, 4465], [6, 11493], [1296, 244], [1296, 2302], [1115, 576], [6, 10950], [28, 3599], [6, 1036], [28, 1296], [6, 28], [28, 4911], [28, 217], [28, 11198], [1115, 8444], [964, 6141], [1863, 2534], [1863, 2753], [964, 4923], [1296, 7765], [5098, 964], [6, 9779], [964, 7085], [964, 2007], [7018, 5098], [28, 5263], [5098, 8600], [3953, 11860], [6, 10004], [964, 9305], [28, 3301], [1296, 8286], [5098, 1144], [1863, 5839], [1296, 10212], [6, 2242], [1863, 4355], [1115, 8146], [6, 67], [1296, 2276], [1296, 11895], [1863, 1442], [3953, 10812], [1296, 11723], [964, 5389], [7018, 1645], [964, 6], [3953, 10609], [28, 7759], [1296, 5558], [28, 1484], [7018, 6916], [964, 1826], [5098, 5048], [964, 6714], [28, 3836], [3953, 4054], [1863, 4929], [28, 1496], [7018, 9845], [1863, 9895], [1296, 7516], [28, 6073], [28, 8339], [28, 9059], [1296, 6], [1863, 5098], [6, 6380], [7018, 2892], [1863, 964], [6, 2976], [6, 7018], [3953, 650], [1863, 5227], [1863, 9919], [3953, 9018], [1115, 1863], [5098, 9244], [1115, 1504], [28, 6], [6, 3254], [1863, 11601], [1115, 4277], [964, 28], [1296, 696], [7018, 1269], [1296, 3209], [1115, 3591], [964, 6019], [1296, 2803], [1863, 10255], [1863, 7365], [1863, 11772], [5098, 2073], [6, 5380], [6, 7677], [7018, 4160], [28, 1115], [28, 2441], [1296, 28], [1296, 1100], [28, 12106], [1296, 3928], [1115, 6], [1115, 11070], [964, 6225], [28, 8505], [1115, 1400], [6, 11186], [1863, 5764], [964, 6347], [28, 7778], [1115, 3953], [964, 10601], [28, 10703], [1115, 3352], [1296, 9290], [1863, 1115], [3953, 6571], [6, 5098], [1863, 6], [5098, 8051], [964, 4763], [964, 7018], [7018, 989], [5098, 9459], [1115, 8369], [964, 9533], [964, 10901], [7018, 9329], [1115, 1227], [5098, 7018], [6, 9336], [28, 2077], [1863, 13], [1863, 667], [5098, 2150], [7018, 43], [1296, 4029], [3953, 11679], [1115, 544], [1115, 303], [1115, 7463], [3953, 1115], [1296, 7421], [1296, 1098], [1863, 6421], [1863, 8300], [28, 1863], [28, 5366], [1115, 28], [3953, 6], [6, 2994], [1863, 9505], [6, 7530], [1296, 6144], [5098, 8118], [1863, 378], [3953, 5789], [1296, 7442], [3953, 4908], [6, 8556], [3953, 6478], [5098, 3416], [964, 11370], [964, 840], [3953, 4054], [1296, 5437], [964, 1863], [1296, 964], [1296, 1577], [1115, 11284], [7018, 5870], [6, 8311], [5098, 11808], [1296, 9455], [28, 964], [1115, 11681], [5098, 3986], [5098, 12172], [5098, 6], [1863, 3780], [6, 2479], [6, 564], [964, 5098], [5098, 2537], [1115, 9842], [5098, 3317], [1296, 11126], [6, 1619], [1115, 8921], [6, 1933], [6, 11177], [7018, 11008], [3953, 10598], [6, 8968], [6, 11171], [28, 5574], [1863, 3853], [28, 1900], [1115, 4319], [7018, 964], [1115, 10572], [1296, 7018], [28, 3953], [28, 3672], [3953, 8168], [6, 4264], [1863, 4062], [7018, 6732], [964, 11115], [7018, 4429], [6, 10661], [28, 3349], [28, 4822], [1863, 6687], [1296, 6510], [1296, 1209], [1296, 5883], [5098, 6490], [7018, 5742], [1863, 8310], [964, 7374], [1863, 3953], [28, 5843], [6, 7838], [1296, 4258], [1115, 1296], [964, 5256], [7018, 1455], [7018, 10437], [1115, 3684], [964, 5098], [3953, 28], [1863, 614], [1863, 10046], [964, 5571], [964, 6540], [1863, 5648], [1863, 4413], [964, 1296], [28, 2434], [5098, 3536], [6, 5717], [6, 6948], [3953, 7738], [1863, 1810], [6, 7826], [1863, 28], [1296, 2688], [6, 4381], [3953, 10627], [1115, 9737], [5098, 4271], [964, 11052], [964, 8655], [5098, 889], [6, 1360], [964, 2734], [1296, 1428], [964, 31], [7018, 4984], [1115, 1762], [7018, 5849]]\n",
      "-------- labels ---\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "-------- length of labels ---\n",
      "312\n",
      "------ x --------\n",
      "[array([5098,    6,    6, 5098, 1296, 1863, 1863, 3953,    6,    6, 1863,\n",
      "        964, 3953, 1863, 1296, 5098, 7018, 1115,    6, 3953,    6,    6,\n",
      "       7018,    6, 5098, 1296,    6, 5098,    6, 1296, 1296,    6, 5098,\n",
      "         28, 7018, 5098, 1115, 1863,   28,    6,  964, 1296, 1296, 3953,\n",
      "         28, 5098, 7018, 1863, 3953,  964,    6, 1296, 1296, 1115,    6,\n",
      "         28,    6,   28,    6,   28,   28,   28, 1115,  964, 1863, 1863,\n",
      "        964, 1296, 5098,    6,  964,  964, 7018,   28, 5098, 3953,    6,\n",
      "        964,   28, 1296, 5098, 1863, 1296,    6, 1863, 1115,    6, 1296,\n",
      "       1296, 1863, 3953, 1296,  964, 7018,  964, 3953,   28, 1296,   28,\n",
      "       7018,  964, 5098,  964,   28, 3953, 1863,   28, 7018, 1863, 1296,\n",
      "         28,   28,   28, 1296, 1863,    6, 7018, 1863,    6,    6, 3953,\n",
      "       1863, 1863, 3953, 1115, 5098, 1115,   28,    6, 1863, 1115,  964,\n",
      "       1296, 7018, 1296, 1115,  964, 1296, 1863, 1863, 1863, 5098,    6,\n",
      "          6, 7018,   28,   28, 1296, 1296,   28, 1296, 1115, 1115,  964,\n",
      "         28, 1115,    6, 1863,  964,   28, 1115,  964,   28, 1115, 1296,\n",
      "       1863, 3953,    6, 1863, 5098,  964,  964, 7018, 5098, 1115,  964,\n",
      "        964, 7018, 1115, 5098,    6,   28, 1863, 1863, 5098, 7018, 1296,\n",
      "       3953, 1115, 1115, 1115, 3953, 1296, 1296, 1863, 1863,   28,   28,\n",
      "       1115, 3953,    6, 1863,    6, 1296, 5098, 1863, 3953, 1296, 3953,\n",
      "          6, 3953, 5098,  964,  964, 3953, 1296,  964, 1296, 1296, 1115,\n",
      "       7018,    6, 5098, 1296,   28, 1115, 5098, 5098, 5098, 1863,    6,\n",
      "          6,  964, 5098, 1115, 5098, 1296,    6, 1115,    6,    6, 7018,\n",
      "       3953,    6,    6,   28, 1863,   28, 1115, 7018, 1115, 1296,   28,\n",
      "         28, 3953,    6, 1863, 7018,  964, 7018,    6,   28,   28, 1863,\n",
      "       1296, 1296, 1296, 5098, 7018, 1863,  964, 1863,   28,    6, 1296,\n",
      "       1115,  964, 7018, 7018, 1115,  964, 3953, 1863, 1863,  964,  964,\n",
      "       1863, 1863,  964,   28, 5098,    6,    6, 3953, 1863,    6, 1863,\n",
      "       1296,    6, 3953, 1115, 5098,  964,  964, 5098,    6,  964, 1296,\n",
      "        964, 7018, 1115, 7018]), array([ 7923, 11838,  4223,  1296,  5098, 12109,  6308,  5242,  1115,\n",
      "        6201,  8423,  2063,  1863,  1296,   742, 10167,     6,  8097,\n",
      "        3953,  1260,  6202,  1863,   208,  4106,  1863,  1863,  3968,\n",
      "        9750,  1296,  1115,  3001,   964, 11569,  4770,  1296,  6606,\n",
      "        9708,  8220,   372,  8437, 12037,  3121,  1330,  4356,  3614,\n",
      "       10392,  5668,   851, 10478,  4465, 11493,   244,  2302,   576,\n",
      "       10950,  3599,  1036,  1296,    28,  4911,   217, 11198,  8444,\n",
      "        6141,  2534,  2753,  4923,  7765,   964,  9779,  7085,  2007,\n",
      "        5098,  5263,  8600, 11860, 10004,  9305,  3301,  8286,  1144,\n",
      "        5839, 10212,  2242,  4355,  8146,    67,  2276, 11895,  1442,\n",
      "       10812, 11723,  5389,  1645,     6, 10609,  7759,  5558,  1484,\n",
      "        6916,  1826,  5048,  6714,  3836,  4054,  4929,  1496,  9845,\n",
      "        9895,  7516,  6073,  8339,  9059,     6,  5098,  6380,  2892,\n",
      "         964,  2976,  7018,   650,  5227,  9919,  9018,  1863,  9244,\n",
      "        1504,     6,  3254, 11601,  4277,    28,   696,  1269,  3209,\n",
      "        3591,  6019,  2803, 10255,  7365, 11772,  2073,  5380,  7677,\n",
      "        4160,  1115,  2441,    28,  1100, 12106,  3928,     6, 11070,\n",
      "        6225,  8505,  1400, 11186,  5764,  6347,  7778,  3953, 10601,\n",
      "       10703,  3352,  9290,  1115,  6571,  5098,     6,  8051,  4763,\n",
      "        7018,   989,  9459,  8369,  9533, 10901,  9329,  1227,  7018,\n",
      "        9336,  2077,    13,   667,  2150,    43,  4029, 11679,   544,\n",
      "         303,  7463,  1115,  7421,  1098,  6421,  8300,  1863,  5366,\n",
      "          28,     6,  2994,  9505,  7530,  6144,  8118,   378,  5789,\n",
      "        7442,  4908,  8556,  6478,  3416, 11370,   840,  4054,  5437,\n",
      "        1863,   964,  1577, 11284,  5870,  8311, 11808,  9455,   964,\n",
      "       11681,  3986, 12172,     6,  3780,  2479,   564,  5098,  2537,\n",
      "        9842,  3317, 11126,  1619,  8921,  1933, 11177, 11008, 10598,\n",
      "        8968, 11171,  5574,  3853,  1900,  4319,   964, 10572,  7018,\n",
      "        3953,  3672,  8168,  4264,  4062,  6732, 11115,  4429, 10661,\n",
      "        3349,  4822,  6687,  6510,  1209,  5883,  6490,  5742,  8310,\n",
      "        7374,  3953,  5843,  7838,  4258,  1296,  5256,  1455, 10437,\n",
      "        3684,  5098,    28,   614, 10046,  5571,  6540,  5648,  4413,\n",
      "        1296,  2434,  3536,  5717,  6948,  7738,  1810,  7826,    28,\n",
      "        2688,  4381, 10627,  9737,  4271, 11052,  8655,   889,  1360,\n",
      "        2734,  1428,    31,  4984,  1762,  5849])]\n",
      "------- y -------\n",
      "[0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "----------------\n",
      "----------------\n",
      "\t0/1: 0.693224430084\t0.480769276619\t0.012821 sec\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "import time\n",
    "\n",
    "t2s = tokenizer.texts_to_sequences(corpus[:1])\n",
    "len_t2s = len(t2s)\n",
    "\n",
    "for cur_epoch in range(epochs):\n",
    "    loss = 0.\n",
    "    accuracy = 0.\n",
    "\n",
    "    start_time = time.time()        \n",
    "    for i, doc in enumerate(t2s):\n",
    "        print('------ doc -------')\n",
    "        print(doc)\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=4, negative_samples=5., shuffle=True)\n",
    "        print('------- data -----')\n",
    "        print(data)\n",
    "        print('-------- labels ---')\n",
    "        print(labels)\n",
    "        print('-------- length of labels ---')\n",
    "        print(len(labels))\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        print('------ x --------')\n",
    "        print(x)\n",
    "        print('------- y -------')\n",
    "        print(y)\n",
    "        print('----------------')\n",
    "        print('----------------')\n",
    "        if x:\n",
    "            # print(SkipGram.train_on_batch(x, y))\n",
    "            train_result = SkipGram.train_on_batch(x, y)\n",
    "            loss += train_result[0]\n",
    "            accuracy += train_result[1]\n",
    "    \n",
    "    avg_loss = loss / len_t2s\n",
    "    avg_acc = accuracy / len_t2s\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(\"\\t%d/%d: %s\\t%s\\t%f sec\" % (cur_epoch, epochs, avg_loss, avg_acc, duration))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save weights...\n"
     ]
    }
   ],
   "source": [
    "print(\"Save weights...\")\n",
    "vector_filename = 'vectors_notebook.txt'\n",
    "f = open(vector_filename ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim_embedddings))\n",
    "vectors = SkipGram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "def most_similar(positive=[], negative=[], topn=20):\n",
    "    w2v = gensim.models.KeyedVectors.load_word2vec_format(vector_filename, binary=False)\n",
    "    for v in w2v.most_similar(positive=positive, negative=negative):\n",
    "        print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for queen...\n",
      "('king', 0.7746679782867432)\n",
      "('hearts', 0.7561852335929871)\n",
      "('tarts', 0.749153733253479)\n",
      "('suppressed', 0.7379329204559326)\n",
      "('mock', 0.736181378364563)\n",
      "('took', 0.73515385389328)\n",
      "('march', 0.7342950105667114)\n",
      "('white', 0.7261685132980347)\n",
      "('lobster', 0.7245875597000122)\n",
      "('end', 0.7231849431991577)\n",
      "Check for alice...\n",
      "('thought', 0.6647700071334839)\n",
      "('glad', 0.658423125743866)\n",
      "('curious', 0.6461477279663086)\n",
      "('it’s', 0.6376360654830933)\n",
      "('wasn’t', 0.6288649439811707)\n",
      "('i’m', 0.6193730235099792)\n",
      "('remarked', 0.6177995204925537)\n",
      "('‘but', 0.6165159940719604)\n",
      "('certainly', 0.6160486340522766)\n",
      "('she’ll', 0.6087310314178467)\n",
      "Check for the...\n",
      "('queen', 0.7193769216537476)\n",
      "('of', 0.6276025772094727)\n",
      "('other', 0.5941148996353149)\n",
      "('king', 0.5940526723861694)\n",
      "('by', 0.5509505271911621)\n",
      "('with', 0.5400514602661133)\n",
      "('from', 0.5364124774932861)\n",
      "('those', 0.5277484655380249)\n",
      "('tax', 0.518311083316803)\n",
      "('owner', 0.5155541300773621)\n",
      "Check for king-he+she...\n",
      "('beginning', 0.4409085214138031)\n",
      "('found', 0.4400646984577179)\n",
      "('read', 0.41559773683547974)\n",
      "('very', 0.40099459886550903)\n",
      "('was', 0.3963632881641388)\n",
      "('pool', 0.38870102167129517)\n",
      "('still', 0.38401132822036743)\n",
      "('problem', 0.383579820394516)\n",
      "('hall', 0.3832961916923523)\n",
      "('frightened', 0.3801344037055969)\n"
     ]
    }
   ],
   "source": [
    "print(\"Check for queen...\")\n",
    "most_similar(positive=['queen'], topn=10)\n",
    "print(\"Check for alice...\")\n",
    "most_similar(positive=['alice'], topn=10)\n",
    "print(\"Check for the...\")\n",
    "most_similar(positive=['the'], topn=10)\n",
    "print(\"Check for king-he+she...\")\n",
    "most_similar(positive=['king', 'she'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
