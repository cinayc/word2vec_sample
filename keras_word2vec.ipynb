{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "\n",
    "import codecs\n",
    "import time\n",
    "np.random.seed(13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/junsoo/PycharmProjects/word2vec_sample/text8'\n",
    "path = '/home/junsoo/PycharmProjects/word2vec_sample/text_sample'\n",
    "corpus = codecs.open(path, \"r\", encoding='utf-8', errors='ignore').read()\n",
    "words = corpus.split()\n",
    "corpus = []\n",
    "sentence = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english \n",
      "\n",
      "revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to \n",
      "\n",
      "describe any act that used violent means to destroy the organization of society it has also been taken up as \n",
      "\n",
      "a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king \n",
      "\n",
      "anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing \n",
      "\n",
      "interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly \n",
      "\n",
      "the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a \n",
      "\n",
      "harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institutions anarchists advocate \n",
      "\n",
      "Vocabulary size: 105\n"
     ]
    }
   ],
   "source": [
    "# word_count_per_sentence = 100\n",
    "word_count_per_sentence = 20\n",
    "\n",
    "for idx,word in enumerate(words):\n",
    "    idx += 1\n",
    "    sentence += word + ' '\n",
    "    if idx % word_count_per_sentence == 0:\n",
    "        corpus.append(sentence.strip())\n",
    "        print(sentence+'\\n')\n",
    "        sentence = ''\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\r\\t\\n‘“')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %s' % vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 1, 128)        13440       input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 1, 128)        13440       input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_3 (Dot)                      (None, 1, 1)          0           embedding_5[0][0]                \n",
      "                                                                   embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 1)             0           dot_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 1)             0           reshape_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 26,880\n",
      "Trainable params: 26,880\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim_embedddings = 128\n",
    "\n",
    "# inputs\n",
    "word_inputs = Input(shape=(1,), dtype='int32')\n",
    "word_vector = Embedding(vocab_size, dim_embedddings, embeddings_initializer='glorot_uniform')(word_inputs)\n",
    "\n",
    "# context\n",
    "context_inputs = Input(shape=(1,), dtype='int32')\n",
    "context_vector = Embedding(vocab_size, dim_embedddings, embeddings_initializer='glorot_uniform')(context_inputs)\n",
    "\n",
    "output_layer = Dot(axes=2)([word_vector, context_vector])\n",
    "output_layer = Reshape((1,), input_shape=(1, 1))(output_layer)\n",
    "output_layer = Activation('sigmoid')(output_layer)\n",
    "\n",
    "SkipGram = Model(inputs=[word_inputs, context_inputs], outputs=output_layer)\n",
    "SkipGram.summary()\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "t2s = tokenizer.texts_to_sequences(corpus)\n",
    "len_t2s = len(t2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1/2: 0.686061151326\t0.56696433574\t[0.049660 sec]\n",
      "\t2/2: 0.678839303553\t0.616666741669\t[0.061425 sec]\n"
     ]
    }
   ],
   "source": [
    "for cur_epoch in range(epochs):\n",
    "    loss = 0.\n",
    "    accuracy = 0.\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, doc in enumerate(t2s):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=vocab_size, window_size=4, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            train_result = SkipGram.train_on_batch(x, y)\n",
    "            loss += train_result[0]\n",
    "            accuracy += train_result[1]\n",
    "\n",
    "        # if i % 10000 == 0:\n",
    "        #     print(\"\\t%d/%d: %s\\t%s\" % (i + 1, len_t2s, loss, accuracy))\n",
    "\n",
    "    avg_loss = loss / len_t2s\n",
    "    avg_acc = accuracy / len_t2s\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    print(\"\\t%d/%d: %s\\t%s\\t[%f sec]\" % (cur_epoch+1, epochs, avg_loss, avg_acc, duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save weights...\n"
     ]
    }
   ],
   "source": [
    "print(\"Save weights...\")\n",
    "vector_filename = 'vectors_notebook.txt'\n",
    "f = open(vector_filename ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size - 1, dim_embedddings))\n",
    "vectors = SkipGram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
