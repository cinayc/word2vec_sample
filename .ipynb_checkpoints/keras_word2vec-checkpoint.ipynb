{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "import gensim\n",
    "import codecs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "# path = get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "path = '/home/junsoo/PycharmProjects/word2vec_sample/corpus.txt'\n",
    "corpus = codecs.open(path, \"r\", encoding='utf-8', errors='ignore').readlines()\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12195"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(corpus[0:10])\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\r\\t\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "V = len(tokenizer.word_index) + 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_embedddings = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "word_inputs = Input(shape=(1,), dtype='int32')\n",
    "w = Embedding(V, dim_embedddings)(word_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "(?, 1, 128)\n",
      "(?, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# context\n",
    "context_inputs = Input(shape=(1,), dtype='int32')\n",
    "context  = Embedding(V, dim_embedddings)(context_inputs)\n",
    "print(context_inputs.shape)\n",
    "print(context.shape)\n",
    "output_layer = Dot(axes=2)([w, context])\n",
    "\n",
    "print(output_layer.shape)\n",
    "output_layer = Reshape((1,), input_shape=(1, 1))(output_layer)\n",
    "output_layer = Activation('sigmoid')(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 1, 128)        1560960     input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 1, 128)        1560960     input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dot_9 (Dot)                      (None, 1, 1)          0           embedding_13[0][0]               \n",
      "                                                                   embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 1)             0           dot_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 1)             0           reshape_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3,121,920\n",
      "Trainable params: 3,121,920\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SkipGram = Model(inputs=[word_inputs, context_inputs], outputs=output_layer)\n",
    "SkipGram.summary()\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "[7018, 5098, 964, 1296, 6, 1863, 28, 1115, 3953]\n",
      "----------------\n",
      "[[7018, 5098], [7018, 964], [5098, 7018], [5098, 964], [5098, 1296], [964, 7018], [964, 5098], [964, 1296], [964, 6], [1296, 5098], [1296, 964], [1296, 6], [1296, 1863], [6, 964], [6, 1296], [6, 1863], [6, 28], [1863, 1296], [1863, 6], [1863, 28], [1863, 1115], [28, 6], [28, 1863], [28, 1115], [28, 3953], [1115, 1863], [1115, 28], [1115, 3953], [3953, 28], [3953, 1115], [6, 5326], [1296, 5528], [7018, 9905], [1863, 5903], [1296, 7329], [6, 2947], [5098, 1143], [1115, 2654], [6, 6974], [1115, 2138], [6, 6494], [28, 7361], [964, 11944], [1115, 9865], [7018, 3601], [964, 11339], [1296, 10932], [1863, 11756], [3953, 9605], [28, 1005], [1863, 9457], [1296, 963], [3953, 8206], [1863, 9156], [28, 9967], [5098, 6404], [28, 5013], [5098, 2294], [964, 4763], [964, 7569]]\n",
      "----------------\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------------\n",
      "[array([7018, 7018, 5098, 5098, 5098,  964,  964,  964,  964, 1296, 1296,\n",
      "       1296, 1296,    6,    6,    6,    6, 1863, 1863, 1863, 1863,   28,\n",
      "         28,   28,   28, 1115, 1115, 1115, 3953, 3953,    6, 1296, 7018,\n",
      "       1863, 1296,    6, 5098, 1115,    6, 1115,    6,   28,  964, 1115,\n",
      "       7018,  964, 1296, 1863, 3953,   28, 1863, 1296, 3953, 1863,   28,\n",
      "       5098,   28, 5098,  964,  964]), array([ 5098,   964,  7018,   964,  1296,  7018,  5098,  1296,     6,\n",
      "        5098,   964,     6,  1863,   964,  1296,  1863,    28,  1296,\n",
      "           6,    28,  1115,     6,  1863,  1115,  3953,  1863,    28,\n",
      "        3953,    28,  1115,  5326,  5528,  9905,  5903,  7329,  2947,\n",
      "        1143,  2654,  6974,  2138,  6494,  7361, 11944,  9865,  3601,\n",
      "       11339, 10932, 11756,  9605,  1005,  9457,   963,  8206,  9156,\n",
      "        9967,  6404,  5013,  2294,  4763,  7569])]\n",
      "----------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "----------------\n",
      "----------------\n",
      "\t0/1: 0.692833662033\t0.58333337307\t0.011267 sec\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "import time\n",
    "\n",
    "t2s = tokenizer.texts_to_sequences(corpus[:1])\n",
    "len_t2s = len(t2s)\n",
    "\n",
    "for cur_epoch in range(epochs):\n",
    "    loss = 0.\n",
    "    accuracy = 0.\n",
    "\n",
    "    start_time = time.time()        \n",
    "    for i, doc in enumerate(t2s):\n",
    "        print('----------------')\n",
    "        print(doc)\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=2, negative_samples=5., shuffle=False)\n",
    "        print('----------------')\n",
    "        print(data)\n",
    "        print('----------------')\n",
    "        print(labels)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        print('----------------')\n",
    "        print(x)\n",
    "        print('----------------')\n",
    "        print(y)\n",
    "        print('----------------')\n",
    "        print('----------------')\n",
    "        if x:\n",
    "            # print(SkipGram.train_on_batch(x, y))\n",
    "            train_result = SkipGram.train_on_batch(x, y)\n",
    "            loss += train_result[0]\n",
    "            accuracy += train_result[1]\n",
    "    \n",
    "    avg_loss = loss / len_t2s\n",
    "    avg_acc = accuracy / len_t2s\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(\"\\t%d/%d: %s\\t%s\\t%f sec\" % (cur_epoch, epochs, avg_loss, avg_acc, duration))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "num_negative_samples = 30 * 5\n",
    "print(num_negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save weights...\n"
     ]
    }
   ],
   "source": [
    "print(\"Save weights...\")\n",
    "vector_filename = 'vectors_notebook.txt'\n",
    "f = open(vector_filename ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim_embedddings))\n",
    "vectors = SkipGram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "def most_similar(positive=[], negative=[], topn=20):\n",
    "    w2v = gensim.models.KeyedVectors.load_word2vec_format(vector_filename, binary=False)\n",
    "    for v in w2v.most_similar(positive=positive, negative=negative):\n",
    "        print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for queen...\n",
      "('king', 0.7746679782867432)\n",
      "('hearts', 0.7561852335929871)\n",
      "('tarts', 0.749153733253479)\n",
      "('suppressed', 0.7379329204559326)\n",
      "('mock', 0.736181378364563)\n",
      "('took', 0.73515385389328)\n",
      "('march', 0.7342950105667114)\n",
      "('white', 0.7261685132980347)\n",
      "('lobster', 0.7245875597000122)\n",
      "('end', 0.7231849431991577)\n",
      "Check for alice...\n",
      "('thought', 0.6647700071334839)\n",
      "('glad', 0.658423125743866)\n",
      "('curious', 0.6461477279663086)\n",
      "('it’s', 0.6376360654830933)\n",
      "('wasn’t', 0.6288649439811707)\n",
      "('i’m', 0.6193730235099792)\n",
      "('remarked', 0.6177995204925537)\n",
      "('‘but', 0.6165159940719604)\n",
      "('certainly', 0.6160486340522766)\n",
      "('she’ll', 0.6087310314178467)\n",
      "Check for the...\n",
      "('queen', 0.7193769216537476)\n",
      "('of', 0.6276025772094727)\n",
      "('other', 0.5941148996353149)\n",
      "('king', 0.5940526723861694)\n",
      "('by', 0.5509505271911621)\n",
      "('with', 0.5400514602661133)\n",
      "('from', 0.5364124774932861)\n",
      "('those', 0.5277484655380249)\n",
      "('tax', 0.518311083316803)\n",
      "('owner', 0.5155541300773621)\n",
      "Check for king-he+she...\n",
      "('beginning', 0.4409085214138031)\n",
      "('found', 0.4400646984577179)\n",
      "('read', 0.41559773683547974)\n",
      "('very', 0.40099459886550903)\n",
      "('was', 0.3963632881641388)\n",
      "('pool', 0.38870102167129517)\n",
      "('still', 0.38401132822036743)\n",
      "('problem', 0.383579820394516)\n",
      "('hall', 0.3832961916923523)\n",
      "('frightened', 0.3801344037055969)\n"
     ]
    }
   ],
   "source": [
    "print(\"Check for queen...\")\n",
    "most_similar(positive=['queen'], topn=10)\n",
    "print(\"Check for alice...\")\n",
    "most_similar(positive=['alice'], topn=10)\n",
    "print(\"Check for the...\")\n",
    "most_similar(positive=['the'], topn=10)\n",
    "print(\"Check for king-he+she...\")\n",
    "most_similar(positive=['king', 'she'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
